import os
import streamlit as st
import pandas as pd
import time
import sys

# --- 1. í˜ì´ì§€ ê¸°ë³¸ ì„¤ì • (ê°€ì¥ ë¨¼ì € í˜¸ì¶œ) ---
st.set_page_config(
    page_title="ì„¸ë´‡ì´ ğŸ¤–",
    page_icon="ğŸ§¾",
    layout="centered",
)

# --- 2. ê¸°ë³¸ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
# ChromaDB ì‚¬ìš©ì„ ìœ„í•œ pysqlite3 ì„¤ì • (Streamlit Cloud ë°°í¬ ì‹œ í•„ìš”)
if sys.version_info.major == 3 and sys.version_info.minor >= 10:
    try:
        __import__('pysqlite3')
        sys.modules['sqlite3'] = sys.modules.get('pysqlite3')
    except ImportError:
        pass

from langchain_community.document_loaders import PyPDFLoader, UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- 3. API í‚¤ ì„¤ì • (ë³´ì•ˆ ê°•í™”) ---
try:
    os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
except (KeyError, FileNotFoundError):
    st.error("ğŸš¨ OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .streamlit/secrets.toml íŒŒì¼ì— í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()


# --- 4. LangChain ë°±ì—”ë“œ í•¨ìˆ˜ ---

@st.cache_resource
def load_and_split_documents(file_paths):
    """ì—¬ëŸ¬ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¶„í• í•˜ëŠ” í•¨ìˆ˜"""
    all_pages = []
    for file_path in file_paths:
        if file_path.endswith('.pdf'):
            loader = PyPDFLoader(file_path)
            all_pages.extend(loader.load_and_split())
        else:
            loader = UnstructuredFileLoader(file_path)
            all_pages.extend(loader.load_and_split())
    return all_pages

@st.cache_resource
def get_vectorstore(_docs):
    """ë¬¸ì„œë“¤ë¡œë¶€í„° ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•˜ê±°ë‚˜ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    split_docs = text_splitter.split_documents(_docs)
    
    vectorstore = Chroma.from_documents(
        documents=split_docs,
        embedding=OpenAIEmbeddings(model='text-embedding-3-small')
    )
    return vectorstore

def format_docs(docs):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í¬ë§·í•˜ëŠ” í•¨ìˆ˜"""
    return "\n\n".join(doc.page_content for doc in docs)

@st.cache_resource
def build_rag_chain():
    """RAG ì²´ì¸ì„ ë¹Œë“œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        base_docs_paths = [
            os.path.join("data", "2025.1ê¸° í™•ì • ë¶€ê°€ê°€ì¹˜ì„¸ ì‹ ê³ ì•ˆë‚´ ë§¤ë‰´ì–¼.pdf"),
            os.path.join("data", "2024ë…„ ì œ2ê¸° í™•ì • ë¶€ê°€ê°€ì¹˜ì„¸ ì‹ ê³ ì•ˆë‚´.pdf"),
            os.path.join("data", "ë¶€ê°€ê°€ì¹˜ì„¸_êµ­ì„¸ì²­_ìœ ê¶Œí•´ì„ì‚¬ë¡€.pdf"),
            os.path.join("data", "ë¶€ê°€ê°€ì¹˜ì„¸_ì‹¤ë¬´ì‚¬ë¡€.pdf")
        ]
        base_documents = load_and_split_documents(base_docs_paths)
        vectorstore = get_vectorstore(base_documents)
        retriever = vectorstore.as_retriever()
    except Exception as e:
        st.error(f"ğŸš¨ ê¸°ë³¸ PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        st.info("'data' í´ë”ì— PDF íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()


    qa_system_prompt = """
    [ì§€ì‹œì‚¬í•­]
    ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë¶€ê°€ê°€ì¹˜ì„¸ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì „ë¬¸ Q&A ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³ , ìƒì„¸í•˜ë©°, ë²•ë¥ ì  ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
    ë§Œì•½ ì‚¬ìš©ìê°€ [ì²¨ë¶€ëœ íŒŒì¼ ë‚´ìš©]ì„ ì œê³µí•˜ë©´, ë°˜ë“œì‹œ ê·¸ ë‚´ìš©ì„ ìµœìš°ì„ ìœ¼ë¡œ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
    
    [ë‹µë³€ ìƒì„± ì›ì¹™]
    1. **ì •í™•ì„±:** ë°˜ë“œì‹œ ëŒ€í•œë¯¼êµ­ ë¶€ê°€ê°€ì¹˜ì„¸ë²• ë° ê´€ë ¨ ê·œì •ì— ë¶€í•©í•˜ëŠ” ë‚´ìš©ë§Œì„ ë‹µë³€í•©ë‹ˆë‹¤.
    2. **êµ¬ì²´ì„±:** ì¶”ìƒì ì¸ ë‹µë³€ ëŒ€ì‹  êµ¬ì²´ì ì¸ ìƒí™©ê³¼ ì—°ê²°í•˜ì—¬ ì„¤ëª…í•˜ê³  ì˜ˆì‹œë¥¼ ë“¤ì–´ ì´í•´ë¥¼ ë•ìŠµë‹ˆë‹¤.
    3. **ê·¼ê±° ì œì‹œ:** ë‹¨ìˆœíˆ "ë„¤/ì•„ë‹ˆì˜¤"ë¡œ ëë‚˜ëŠ” ë‹µë³€ì´ ì•„ë‹Œ, ì™œ ê·¸ëŸ°ì§€ ì´ìœ ì™€ ê·¼ê±°ë¥¼ í•¨ê»˜ ì œì‹œí•©ë‹ˆë‹¤.
    4. **ìµœì‹ ì„±:** ê¸°ë³¸ ì§€ì‹ì€ 25ë…„ ë§¤ë‰´ì–¼ì„ ë”°ë¥´ë˜, ê³¼ê±°ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì—ëŠ” 24ë…„ ë§¤ë‰´ì–¼ì„ ì°¸ê³ í•©ë‹ˆë‹¤.
    5. **íŒŒì¼ ë‚´ìš© ìš°ì„ :** ì‚¬ìš©ìê°€ íŒŒì¼ì„ ì²¨ë¶€í–ˆë‹¤ë©´, ê²€ìƒ‰ëœ ì¼ë°˜ ì§€ì‹ë³´ë‹¤ ì²¨ë¶€ íŒŒì¼ì˜ ë‚´ìš©ì„ ìš°ì„ í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.
    
    [ë‹µë³€ êµ¬ì¡°]
    - **í•µì‹¬ ìš”ì•½:** ì§ˆë¬¸ì— ëŒ€í•œ ê°„ê²°í•˜ê³  ì§ì ‘ì ì¸ ë‹µë³€.
    - **ìƒì„¸ ì„¤ëª…:** í•µì‹¬ ìš”ì•½ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ ì¶”ê°€ ì •ë³´.
    - **ì£¼ì˜ì‚¬í•­/ì°¸ê³ :** ì¶”ê°€ì ìœ¼ë¡œ ì•Œì•„ì•¼ í•  ì , ì˜ˆì™¸ ì‚¬í•­, ì „ë¬¸ê°€ ìƒë‹´ ê¶Œìœ  ë“±.
    
    ---
    [ê²€ìƒ‰ëœ ì¼ë°˜ ì§€ì‹]
    {context}
    ---
    """

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        ("human", "{input}"),
    ])

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    
    rag_chain = (
        {"context": retriever | format_docs, "input": RunnablePassthrough()}
        | qa_prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain

# --- 5. ë©”ì¸ ë¡œì§ ---
rag_chain = build_rag_chain()

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "messages" not in st.session_state:
    st.session_state.messages = []
if "file_context" not in st.session_state:
    st.session_state.file_context = None
if "file_name" not in st.session_state:
    st.session_state.file_name = None

# --- ì‚¬ì´ë“œë°” UI êµ¬ì„± ---
with st.sidebar:
    st.title("ğŸ§¾ íŒŒì¼ ì—…ë¡œë“œ")
    st.info("ì„¸ê¸ˆê³„ì‚°ì„œ(XLSX, CSV)ë‚˜ ê´€ë ¨ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    uploaded_file = st.file_uploader(
        "íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.",
        type=['xlsx', 'csv', 'pdf'],
        label_visibility="collapsed"
    )

    if uploaded_file:
        try:
            file_name = uploaded_file.name
            df = None
            
            if file_name.endswith('.xlsx'):
                df = pd.read_excel(uploaded_file)
            elif file_name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            elif file_name.endswith('.pdf'):
                st.warning("PDF íŒŒì¼ì€ í˜„ì¬ ë¯¸ë¦¬ë³´ê¸°ë¥¼ ì§€ì›í•˜ì§€ ì•Šì§€ë§Œ, ë‚´ìš©ì— ëŒ€í•œ ì§ˆë¬¸ì€ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                st.session_state.file_name = file_name
                st.session_state.file_context = "PDF íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. íŒŒì¼ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
            
            if df is not None:
                st.session_state.file_context = df.to_markdown()
                st.session_state.file_name = file_name
                st.subheader("âœ… ì—…ë¡œë“œ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°")
                st.dataframe(df, height=300, use_container_width=True)

        except Exception as e:
            st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            st.session_state.file_context = None
            st.session_state.file_name = None
    
    if st.session_state.file_name:
        if st.button("ğŸ”„ ì—…ë¡œë“œ íŒŒì¼ ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.file_context = None
            st.session_state.file_name = None
            st.success("íŒŒì¼ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì¼ë°˜ ì§ˆë¬¸ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            time.sleep(1)
            st.rerun()

# --- ë©”ì¸ í™”ë©´ UI êµ¬ì„± ---
st.title("ì„¸ë´‡ì´ ğŸ¤–")
st.write("ë¶€ê°€ê°€ì¹˜ì„¸ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤. ì™¼ìª½ ì‚¬ì´ë“œë°”ì— íŒŒì¼ì„ ì˜¬ë ¤ íŠ¹ì • ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.")
st.write("---")

if st.session_state.file_name:
    st.info(f"ğŸ§¾ **'{st.session_state.file_name}' íŒŒì¼ì— ëŒ€í•´ ì§ˆë¬¸í•©ë‹ˆë‹¤.**")
else:
    st.info("ğŸ’¡ **ì¼ë°˜ ë¶€ê°€ê°€ì¹˜ì„¸ ì§ˆë¬¸ ëª¨ë“œì…ë‹ˆë‹¤.**")

# --- ì±„íŒ… ëŒ€í™” ê¸°ë¡ í‘œì‹œ ---
if not st.session_state.messages:
    st.session_state.messages.append({"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¶€ê°€ê°€ì¹˜ì„¸ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”."})

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

# --- ì˜ˆì‹œ ì§ˆë¬¸ ë²„íŠ¼ ---
if len(st.session_state.messages) <= 1:
    st.write("**ì˜ˆì‹œ ì§ˆë¬¸:**")
    cols = st.columns(3)
    example_questions = ["ê°„ì´ê³¼ì„¸ì ë¶€ê°€ì„¸ëŠ”?", "ë§¤ì…ì„¸ì•¡ê³µì œë€?", "ì„¸ê¸ˆê³„ì‚°ì„œ ë°œê¸‰ì˜ë¬´"]
    
    # ì˜ˆì‹œ ë²„íŠ¼ í´ë¦­ ì‹œ ë©”ì‹œì§€ ì¶”ê°€ ë° rerun
    if cols[0].button(example_questions[0], use_container_width=True):
        st.session_state.messages.append({"role": "user", "content": example_questions[0]})
        st.rerun()
    if cols[1].button(example_questions[1], use_container_width=True):
        st.session_state.messages.append({"role": "user", "content": example_questions[1]})
        st.rerun()
    if cols[2].button(example_questions[2], use_container_width=True):
        st.session_state.messages.append({"role": "user", "content": example_questions[2]})
        st.rerun()

# --- ì‚¬ìš©ì ì±„íŒ… ì…ë ¥ ---
if user_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” :)"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.rerun()

# --- ì±—ë´‡ ì‘ë‹µ ìƒì„± ë¡œì§ ---
# ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ì‚¬ìš©ìì˜ ê²ƒì´ê³ , ì•„ì§ ë´‡ì´ ì‘ë‹µí•˜ì§€ ì•Šì•˜ì„ ê²½ìš°ì—ë§Œ ì‹¤í–‰
if st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
    user_prompt = st.session_state.messages[-1]["content"]
    
    final_prompt = user_prompt
    if st.session_state.file_context:
        final_prompt = f"""
[ì²¨ë¶€ëœ íŒŒì¼ ë‚´ìš©]
{st.session_state.file_context}
---
ìœ„ ì²¨ë¶€íŒŒì¼ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]
{user_prompt}
"""
    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
            response = rag_chain.invoke(final_prompt)
            st.write(response)
            # ì‘ë‹µì„ stateì— ì¶”ê°€
            st.session_state.messages.append({"role": "assistant", "content": response})
